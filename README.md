# Deep Learning Study Tutorial

## Optimization

<div align="center">
        <img src="https://github.com/nji3/Deep_Learning_Study_Tutorial/blob/master/readme_images/optim_gd.png" width="400px"</img> 
</div>

Gradient Descent is a very common scheme of optimization algorithms. It is very useful to find the global/local minimum of the loss function we used to do the training of the model. The most foundmental types of gradient descent methods are the batch gradient descent, mini-batch gradient descent and the stochastic gradient descent. The convergence of these optimization algorithms is proven years ago.

For the deep learning model, the stochastic gradient descent and the mini-batch gradient descent become very popular to train the loss function because they are functional as a kind of regularization to the model complexity. People now use very advanced optimization method based on the stochastiv gradient descent with an adaptive learning rate. Adam is one of the most used optimization algorithm in the deep learning field.

## Convolutional Neural Networks

## Autoencoder and VAE(Variational Autoencoder)

### Autoencoder

### Variational Autoencoder

## Generative Adversarial Network

Record the learning materials for thesis

Week 1: The Overview of Gradient Descent Algorithms with momentum

Week 2 to 3: Introductary Study of CNN

Week 4: How Beautiful are these Animals (Simple application of Convnet, Overfitting not finished yet)

Week 5: AutoEncoder on the Human Face images and Image denoising

Week 6: AutoEncoder-Interpolation

Week 7: Autoencoder with Latent Variable Layer and Variational Autoencoder(VAE)

Week 8-9: Tensorflow Intro for VAE and GAN
